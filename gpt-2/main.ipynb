{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 1: Import Libraries and Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 1: Import Libraries and Dataset Preparation\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define category, intent, and NER mappings\n",
    "category_map = {\n",
    "    \"ACCOUNT\": 0,\n",
    "    \"CANCELLATION_FEE\": 1,\n",
    "    \"CONTACT\": 2,\n",
    "    \"DELIVERY\": 3,\n",
    "    \"FEEDBACK\": 4,\n",
    "    \"INVOICE\": 5,\n",
    "    \"ORDER\": 6,\n",
    "    \"PAYMENT\": 7,\n",
    "    \"REFUND\": 8,\n",
    "    \"SHIPPING_ADDRESS\": 9,\n",
    "    \"SUBSCRIPTION\": 10\n",
    "}\n",
    "\n",
    "intent_map = {\n",
    "    \"create_account\": 0,\n",
    "    \"delete_account\": 1,\n",
    "    \"edit_account\": 2,\n",
    "    \"recover_password\": 3,\n",
    "    \"registration_problems\": 4,\n",
    "    \"switch_account\": 5,\n",
    "    \"check_cancellation_fee\": 6,\n",
    "    \"contact_customer_service\": 7,\n",
    "    \"contact_human_agent\": 8,\n",
    "    \"delivery_options\": 9,\n",
    "    \"delivery_period\": 10,\n",
    "    \"complaint\": 11,\n",
    "    \"review\": 12,\n",
    "    \"check_invoice\": 13,\n",
    "    \"get_invoice\": 14,\n",
    "    \"cancel_order\": 15,\n",
    "    \"change_order\": 16,\n",
    "    \"place_order\": 17,\n",
    "    \"track_order\": 18,\n",
    "    \"check_payment_methods\": 19,\n",
    "    \"payment_issue\": 20,\n",
    "    \"check_refund_policy\": 21,\n",
    "    \"get_refund\": 22,\n",
    "    \"track_refund\": 23,\n",
    "    \"change_shipping_address\": 24,\n",
    "    \"set_up_shipping_address\": 25,\n",
    "    \"newsletter_subscription\": 26\n",
    "}\n",
    "\n",
    "ner_map = {\n",
    "    \"O\": 0,  # Outside of a named entity\n",
    "    \"B-ORDER_NUMBER\": 1,  # Beginning of an order number\n",
    "    \"I-ORDER_NUMBER\": 2,  # Inside an order number\n",
    "    \"B-INVOICE_NUMBER\": 3,  # Beginning of an invoice number\n",
    "    \"I-INVOICE_NUMBER\": 4,  # Inside an invoice number\n",
    "    \"B-ONLINE_ORDER_INTERACTION\": 5,  # Beginning of an online order interaction\n",
    "    \"I-ONLINE_ORDER_INTERACTION\": 6,  # Inside an online order interaction\n",
    "    \"B-ONLINE_PAYMENT_INTERACTION\": 7,  # Beginning of an online payment interaction\n",
    "    \"I-ONLINE_PAYMENT_INTERACTION\": 8,  # Inside an online payment interaction\n",
    "    \"B-ONLINE_NAVIGATION_STEP\": 9,  # Beginning of an online navigation step\n",
    "    \"I-ONLINE_NAVIGATION_STEP\": 10,  # Inside an online navigation step\n",
    "    \"B-ONLINE_CUSTOMER_SUPPORT_CHANNEL\": 11,  # Beginning of an online customer support channel\n",
    "    \"I-ONLINE_CUSTOMER_SUPPORT_CHANNEL\": 12,  # Inside an online customer support channel\n",
    "    \"B-PROFILE\": 13,  # Beginning of a profile\n",
    "    \"I-PROFILE\": 14,  # Inside a profile\n",
    "    \"B-PROFILE_TYPE\": 15,  # Beginning of a profile type\n",
    "    \"I-PROFILE_TYPE\": 16,  # Inside a profile type\n",
    "    \"B-SETTINGS\": 17,  # Beginning of settings\n",
    "    \"I-SETTINGS\": 18,  # Inside settings\n",
    "    \"B-ONLINE_COMPANY_PORTAL_INFO\": 19,  # Beginning of online company portal info\n",
    "    \"I-ONLINE_COMPANY_PORTAL_INFO\": 20,  # Inside online company portal info\n",
    "    \"B-DATE\": 21,  # Beginning of a date\n",
    "    \"I-DATE\": 22,  # Inside a date\n",
    "    \"B-DATE_RANGE\": 23,  # Beginning of a date range\n",
    "    \"I-DATE_RANGE\": 24,  # Inside a date range\n",
    "    \"B-SHIPPING_CUT_OFF_TIME\": 25,  # Beginning of a shipping cut-off time\n",
    "    \"I-SHIPPING_CUT_OFF_TIME\": 26,  # Inside a shipping cut-off time\n",
    "    \"B-DELIVERY_CITY\": 27,  # Beginning of a delivery city\n",
    "    \"I-DELIVERY_CITY\": 28,  # Inside a delivery city\n",
    "    \"B-DELIVERY_COUNTRY\": 29,  # Beginning of a delivery country\n",
    "    \"I-DELIVERY_COUNTRY\": 30,  # Inside a delivery country\n",
    "    \"B-SALUTATION\": 31,  # Beginning of a salutation\n",
    "    \"I-SALUTATION\": 32,  # Inside a salutation\n",
    "    \"B-CLIENT_FIRST_NAME\": 33,  # Beginning of a client's first name\n",
    "    \"I-CLIENT_FIRST_NAME\": 34,  # Inside a client's first name\n",
    "    \"B-CLIENT_LAST_NAME\": 35,  # Beginning of a client's last name\n",
    "    \"I-CLIENT_LAST_NAME\": 36,  # Inside a client's last name\n",
    "    \"B-CUSTOMER_SUPPORT_PHONE_NUMBER\": 37,  # Beginning of a customer support phone number\n",
    "    \"I-CUSTOMER_SUPPORT_PHONE_NUMBER\": 38,  # Inside a customer support phone number\n",
    "    \"B-CUSTOMER_SUPPORT_EMAIL\": 39,  # Beginning of a customer support email\n",
    "    \"I-CUSTOMER_SUPPORT_EMAIL\": 40,  # Inside a customer support email\n",
    "    \"B-LIVE_CHAT_SUPPORT\": 41,  # Beginning of live chat support\n",
    "    \"I-LIVE_CHAT_SUPPORT\": 42,  # Inside live chat support\n",
    "    \"B-WEBSITE_URL\": 43,  # Beginning of a website URL\n",
    "    \"I-WEBSITE_URL\": 44,  # Inside a website URL\n",
    "    \"B-UPGRADE_ACCOUNT\": 45,  # Beginning of an upgrade account\n",
    "    \"I-UPGRADE_ACCOUNT\": 46,  # Inside an upgrade account\n",
    "    \"B-ACCOUNT_TYPE\": 47,  # Beginning of an account type\n",
    "    \"I-ACCOUNT_TYPE\": 48,  # Inside an account type\n",
    "    \"B-ACCOUNT_CATEGORY\": 49,  # Beginning of an account category\n",
    "    \"I-ACCOUNT_CATEGORY\": 50,  # Inside an account category\n",
    "    \"B-ACCOUNT_CHANGE\": 51,  # Beginning of an account change\n",
    "    \"I-ACCOUNT_CHANGE\": 52,  # Inside an account change\n",
    "    \"B-PROGRAM\": 53,  # Beginning of a program\n",
    "    \"I-PROGRAM\": 54,  # Inside a program\n",
    "    \"B-REFUND_AMOUNT\": 55,  # Beginning of a refund amount\n",
    "    \"I-REFUND_AMOUNT\": 56,  # Inside a refund amount\n",
    "    \"B-MONEY_AMOUNT\": 57,  # Beginning of a money amount\n",
    "    \"I-MONEY_AMOUNT\": 58,  # Inside a money amount\n",
    "    \"B-STORE_LOCATION\": 59,  # Beginning of a store location\n",
    "    \"I-STORE_LOCATION\": 60  # Inside a store location\n",
    "}\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length, category_map, intent_map, ner_map):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.category_map = category_map\n",
    "        self.intent_map = intent_map\n",
    "        self.ner_map = ner_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]  # Access by row index\n",
    "        text = f\"{row['instruction']} {row['response']}\"  # NER input from instruction + response\n",
    "        category = row['category']\n",
    "        intent = row['intent']\n",
    "\n",
    "        # Tokenize text with padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",  # Padding will use the newly defined pad_token\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Extract NER labels from the text (assuming a function `extract_ner_labels` exists)\n",
    "        ner_labels = extract_ner_labels(text, self.ner_map)\n",
    "        ner_label_ids = [self.ner_map[label] for label in ner_labels]\n",
    "        ner_label_ids += [0] * (self.max_length - len(ner_label_ids))  # Pad NER labels to max_length\n",
    "\n",
    "        # Handle unexpected categories and intents\n",
    "        category_label = self.category_map.get(category, -1)  # Default to -1 if category not found\n",
    "        intent_label = self.intent_map.get(intent, -1)        # Default to -1 if intent not found\n",
    "\n",
    "        # Return inputs and task-specific labels\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"category\": torch.tensor(category_label, dtype=torch.long),  # Map categories to numerical labels\n",
    "            \"intent\": torch.tensor(intent_label, dtype=torch.long),      # Map intents to numerical labels\n",
    "            \"ner_labels\": torch.tensor(ner_label_ids, dtype=torch.long)  # Map NER labels to numerical labels\n",
    "        }\n",
    "\n",
    "def extract_ner_labels(text, ner_map):\n",
    "    # Dummy implementation for extracting NER labels from text\n",
    "    # Replace this with your actual NER extraction logic\n",
    "    words = text.split()\n",
    "    labels = [\"O\"] * len(words)  # Default to \"O\" for all words\n",
    "    return labels\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('../../dataset/Bitext.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 2: DataLoader and Train-Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 2: DataLoader and Train-Validation Split\n",
    "\n",
    "# Assuming df is your full dataset, apply train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos_token for GPT-2\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = CustomDataset(train_df, tokenizer, max_length=128, category_map=category_map, intent_map=intent_map, ner_map=ner_map)\n",
    "val_dataset = CustomDataset(val_df, tokenizer, max_length=128, category_map=category_map, intent_map=intent_map, ner_map=ner_map)\n",
    "\n",
    "# Custom collate function to pad sequences to the same length\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    category = [item['category'] for item in batch]\n",
    "    intent = [item['intent'] for item in batch]\n",
    "    ner_labels = [item['ner_labels'] for item in batch]\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    category = torch.stack(category)\n",
    "    intent = torch.stack(intent)\n",
    "    ner_labels = torch.nn.utils.rnn.pad_sequence(ner_labels, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'category': category,\n",
    "        'intent': intent,\n",
    "        'ner_labels': ner_labels\n",
    "    }\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 3: Model Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of NERModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.0.ln_1.bias', 'gpt2.h.0.ln_1.weight', 'gpt2.h.0.ln_2.bias', 'gpt2.h.0.ln_2.weight', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.1.ln_1.bias', 'gpt2.h.1.ln_1.weight', 'gpt2.h.1.ln_2.bias', 'gpt2.h.1.ln_2.weight', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.1.mlp.c_fc.weight', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.10.ln_1.bias', 'gpt2.h.10.ln_1.weight', 'gpt2.h.10.ln_2.bias', 'gpt2.h.10.ln_2.weight', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.11.ln_1.bias', 'gpt2.h.11.ln_1.weight', 'gpt2.h.11.ln_2.bias', 'gpt2.h.11.ln_2.weight', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.2.ln_1.bias', 'gpt2.h.2.ln_1.weight', 'gpt2.h.2.ln_2.bias', 'gpt2.h.2.ln_2.weight', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.3.ln_1.bias', 'gpt2.h.3.ln_1.weight', 'gpt2.h.3.ln_2.bias', 'gpt2.h.3.ln_2.weight', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.h.4.attn.c_attn.bias', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.4.ln_1.bias', 'gpt2.h.4.ln_1.weight', 'gpt2.h.4.ln_2.bias', 'gpt2.h.4.ln_2.weight', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.5.attn.c_proj.weight', 'gpt2.h.5.ln_1.bias', 'gpt2.h.5.ln_1.weight', 'gpt2.h.5.ln_2.bias', 'gpt2.h.5.ln_2.weight', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.6.attn.c_proj.bias', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.6.ln_1.bias', 'gpt2.h.6.ln_1.weight', 'gpt2.h.6.ln_2.bias', 'gpt2.h.6.ln_2.weight', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.7.ln_1.bias', 'gpt2.h.7.ln_1.weight', 'gpt2.h.7.ln_2.bias', 'gpt2.h.7.ln_2.weight', 'gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.h.7.mlp.c_proj.weight', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.8.ln_1.bias', 'gpt2.h.8.ln_1.weight', 'gpt2.h.8.ln_2.bias', 'gpt2.h.8.ln_2.weight', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.9.attn.c_attn.weight', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.9.ln_1.bias', 'gpt2.h.9.ln_1.weight', 'gpt2.h.9.ln_2.bias', 'gpt2.h.9.ln_2.weight', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.ln_f.bias', 'gpt2.ln_f.weight', 'gpt2.wpe.weight', 'gpt2.wte.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of CategoryModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.0.ln_1.bias', 'gpt2.h.0.ln_1.weight', 'gpt2.h.0.ln_2.bias', 'gpt2.h.0.ln_2.weight', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.1.ln_1.bias', 'gpt2.h.1.ln_1.weight', 'gpt2.h.1.ln_2.bias', 'gpt2.h.1.ln_2.weight', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.1.mlp.c_fc.weight', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.10.ln_1.bias', 'gpt2.h.10.ln_1.weight', 'gpt2.h.10.ln_2.bias', 'gpt2.h.10.ln_2.weight', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.11.ln_1.bias', 'gpt2.h.11.ln_1.weight', 'gpt2.h.11.ln_2.bias', 'gpt2.h.11.ln_2.weight', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.2.ln_1.bias', 'gpt2.h.2.ln_1.weight', 'gpt2.h.2.ln_2.bias', 'gpt2.h.2.ln_2.weight', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.3.ln_1.bias', 'gpt2.h.3.ln_1.weight', 'gpt2.h.3.ln_2.bias', 'gpt2.h.3.ln_2.weight', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.h.4.attn.c_attn.bias', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.4.ln_1.bias', 'gpt2.h.4.ln_1.weight', 'gpt2.h.4.ln_2.bias', 'gpt2.h.4.ln_2.weight', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.5.attn.c_proj.weight', 'gpt2.h.5.ln_1.bias', 'gpt2.h.5.ln_1.weight', 'gpt2.h.5.ln_2.bias', 'gpt2.h.5.ln_2.weight', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.6.attn.c_proj.bias', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.6.ln_1.bias', 'gpt2.h.6.ln_1.weight', 'gpt2.h.6.ln_2.bias', 'gpt2.h.6.ln_2.weight', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.7.ln_1.bias', 'gpt2.h.7.ln_1.weight', 'gpt2.h.7.ln_2.bias', 'gpt2.h.7.ln_2.weight', 'gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.h.7.mlp.c_proj.weight', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.8.ln_1.bias', 'gpt2.h.8.ln_1.weight', 'gpt2.h.8.ln_2.bias', 'gpt2.h.8.ln_2.weight', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.9.attn.c_attn.weight', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.9.ln_1.bias', 'gpt2.h.9.ln_1.weight', 'gpt2.h.9.ln_2.bias', 'gpt2.h.9.ln_2.weight', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.ln_f.bias', 'gpt2.ln_f.weight', 'gpt2.wpe.weight', 'gpt2.wte.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of IntentModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.0.ln_1.bias', 'gpt2.h.0.ln_1.weight', 'gpt2.h.0.ln_2.bias', 'gpt2.h.0.ln_2.weight', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.1.ln_1.bias', 'gpt2.h.1.ln_1.weight', 'gpt2.h.1.ln_2.bias', 'gpt2.h.1.ln_2.weight', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.1.mlp.c_fc.weight', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.10.ln_1.bias', 'gpt2.h.10.ln_1.weight', 'gpt2.h.10.ln_2.bias', 'gpt2.h.10.ln_2.weight', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.11.ln_1.bias', 'gpt2.h.11.ln_1.weight', 'gpt2.h.11.ln_2.bias', 'gpt2.h.11.ln_2.weight', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.2.ln_1.bias', 'gpt2.h.2.ln_1.weight', 'gpt2.h.2.ln_2.bias', 'gpt2.h.2.ln_2.weight', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.3.ln_1.bias', 'gpt2.h.3.ln_1.weight', 'gpt2.h.3.ln_2.bias', 'gpt2.h.3.ln_2.weight', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.h.4.attn.c_attn.bias', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.4.ln_1.bias', 'gpt2.h.4.ln_1.weight', 'gpt2.h.4.ln_2.bias', 'gpt2.h.4.ln_2.weight', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.5.attn.c_proj.weight', 'gpt2.h.5.ln_1.bias', 'gpt2.h.5.ln_1.weight', 'gpt2.h.5.ln_2.bias', 'gpt2.h.5.ln_2.weight', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.6.attn.c_proj.bias', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.6.ln_1.bias', 'gpt2.h.6.ln_1.weight', 'gpt2.h.6.ln_2.bias', 'gpt2.h.6.ln_2.weight', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.7.ln_1.bias', 'gpt2.h.7.ln_1.weight', 'gpt2.h.7.ln_2.bias', 'gpt2.h.7.ln_2.weight', 'gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.h.7.mlp.c_proj.weight', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.8.ln_1.bias', 'gpt2.h.8.ln_1.weight', 'gpt2.h.8.ln_2.bias', 'gpt2.h.8.ln_2.weight', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.9.attn.c_attn.weight', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.9.ln_1.bias', 'gpt2.h.9.ln_1.weight', 'gpt2.h.9.ln_2.bias', 'gpt2.h.9.ln_2.weight', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.ln_f.bias', 'gpt2.ln_f.weight', 'gpt2.wpe.weight', 'gpt2.wte.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IntentModel(\n",
       "  (gpt2): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch 3: Model Setup\n",
    "\n",
    "class NERModel(GPT2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.gpt2 = GPT2Model(config)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "class CategoryModel(GPT2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.gpt2 = GPT2Model(config)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[0][:, -1, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "class IntentModel(GPT2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.gpt2 = GPT2Model(config)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[0][:, -1, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# Initialize the models\n",
    "ner_model = NERModel.from_pretrained(\"gpt2\", num_labels=len(ner_map))  # Use the length of ner_map for num_labels\n",
    "category_model = CategoryModel.from_pretrained(\"gpt2\", num_labels=len(category_map))\n",
    "intent_model = IntentModel.from_pretrained(\"gpt2\", num_labels=len(intent_map))\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device('cpu')\n",
    "ner_model.to(device)\n",
    "category_model.to(device)\n",
    "intent_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 4: Loss Functions and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 4: Loss Functions and Optimizer\n",
    "\n",
    "def compute_loss(category_logits, intent_logits, ner_logits, category_labels, intent_labels, ner_labels):\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    category_loss = loss_fct(category_logits.view(-1, category_logits.size(-1)), category_labels.view(-1))\n",
    "    intent_loss = loss_fct(intent_logits.view(-1, intent_logits.size(-1)), intent_labels.view(-1))\n",
    "    ner_loss = loss_fct(ner_logits.view(-1, ner_logits.size(-1)), ner_labels.view(-1))\n",
    "    return category_loss + intent_loss + ner_loss\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(list(ner_model.parameters()) + list(category_model.parameters()) + list(intent_model.parameters()), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 5: Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:   0%|          | 0/1344 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Batch 5: Training Loop\n",
    "\n",
    "def train_epoch(ner_model, category_model, intent_model, dataloader, optimizer, device):\n",
    "    ner_model.train()\n",
    "    category_model.train()\n",
    "    intent_model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training Batches\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        category_labels = batch['category'].to(device)\n",
    "        intent_labels = batch['intent'].to(device)\n",
    "        ner_labels = batch['ner_labels'].to(device)\n",
    "\n",
    "        # Step 1: Get NER logits (token-level task)\n",
    "        ner_logits = ner_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "        # Step 2: Get category logits (class-level task)\n",
    "        category_logits = category_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "        # Step 3: Get intent logits (class-level task)\n",
    "        intent_logits = intent_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "        # Step 4: Compute loss for this batch\n",
    "        loss = compute_loss(\n",
    "            category_logits=category_logits,\n",
    "            intent_logits=intent_logits,\n",
    "            ner_logits=ner_logits,\n",
    "            category_labels=category_labels,\n",
    "            intent_labels=intent_labels,\n",
    "            ner_labels=ner_labels,\n",
    "        )\n",
    "\n",
    "        # Step 5: Backward pass\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update model weights\n",
    "\n",
    "        # Accumulate total loss\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "    # Return the average loss for the epoch\n",
    "    avg_loss = total_loss / batch_count\n",
    "    return avg_loss\n",
    "\n",
    "# Main Training Loop\n",
    "for epoch in range(3):  # Total number of epochs\n",
    "    print(f\"Epoch {epoch + 1}/3\")\n",
    "    \n",
    "    # Train for one epoch\n",
    "    avg_loss = train_epoch(ner_model, category_model, intent_model, train_dataloader, optimizer, device)\n",
    "    \n",
    "    # Print the average loss for the epoch\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --train_file TRAIN_FILE --eval_file\n",
      "                             EVAL_FILE --model MODEL [--bert_model BERT_MODEL]\n",
      "                             [--xlnet_model XLNET_MODEL]\n",
      "                             [--gpt2_model GPT2_MODEL]\n",
      "                             [--gpt2_classification_type GPT2_CLASSIFICATION_TYPE]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE] [--gpu GPU]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--prob_threshold PROB_THRESHOLD]\n",
      "                             [--max_seq_length MAX_SEQ_LENGTH]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --train_file, --eval_file, --model\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reagan/pytorch_env/pytorch_env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 6: Validation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 6: Validation Loop\n",
    "\n",
    "def validate(ner_model, category_model, intent_model, dataloader, device):\n",
    "    ner_model.eval()\n",
    "    category_model.eval()\n",
    "    intent_model.eval()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating Batches\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            category_labels = batch['category'].to(device)\n",
    "            intent_labels = batch['intent'].to(device)\n",
    "            ner_labels = batch['ner_labels'].to(device)\n",
    "\n",
    "            # Step 1: Get NER logits (token-level task)\n",
    "            ner_logits = ner_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "            # Step 2: Get category logits (class-level task)\n",
    "            category_logits = category_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "            # Step 3: Get intent logits (class-level task)\n",
    "            intent_logits = intent_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "            # Step 4: Compute loss for this batch\n",
    "            loss = compute_loss(\n",
    "                category_logits=category_logits,\n",
    "                intent_logits=intent_logits,\n",
    "                ner_logits=ner_logits,\n",
    "                category_labels=category_labels,\n",
    "                intent_labels=intent_labels,\n",
    "                ner_labels=ner_labels,\n",
    "            )\n",
    "\n",
    "            # Accumulate total loss\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "    # Return the average loss for the validation\n",
    "    avg_loss = total_loss / batch_count\n",
    "    return avg_loss\n",
    "\n",
    "# Validate the models\n",
    "avg_val_loss = validate(ner_model, category_model, intent_model, val_dataloader, device)\n",
    "print(f\"Average Validation Loss: {avg_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 7: Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Batch 7: Save Models and Tokenizer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save the NER model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mner_model\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_to_save_ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Save the Category model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m category_model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_to_save_category_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ner_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Batch 7: Save Models and Tokenizer\n",
    "\n",
    "# Save the NER model\n",
    "ner_model.save_pretrained(\"path_to_save_ner_model\")\n",
    "# Save the Category model\n",
    "category_model.save_pretrained(\"path_to_save_category_model\")\n",
    "# Save the Intent model\n",
    "intent_model.save_pretrained(\"path_to_save_intent_model\")\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"path_to_save_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 8: Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 8: Inference\n",
    "\n",
    "def inference(ner_model, category_model, intent_model, tokenizer, text, device):\n",
    "    # Tokenize text with padding and truncation\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Step 1: Get NER logits (token-level task)\n",
    "    ner_logits = ner_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "    ner_predictions = torch.argmax(ner_logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    # Step 2: Get category logits (class-level task)\n",
    "    category_logits = category_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "    category_prediction = torch.argmax(category_logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    # Step 3: Get intent logits (class-level task)\n",
    "    intent_logits = intent_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "    intent_prediction = torch.argmax(intent_logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    return ner_predictions, category_prediction, intent_prediction\n",
    "\n",
    "# Example usage\n",
    "text = \"Your example text here\"\n",
    "ner_predictions, category_prediction, intent_prediction = inference(ner_model, category_model, intent_model, tokenizer, text, device)\n",
    "print(f\"NER Predictions: {ner_predictions}\")\n",
    "print(f\"Category Prediction: {category_prediction}\")\n",
    "print(f\"Intent Prediction: {intent_prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
